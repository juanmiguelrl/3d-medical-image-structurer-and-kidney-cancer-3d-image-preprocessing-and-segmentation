{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK1JBN2e96IS",
        "outputId": "d682c5b7-5012-43a0-b4e5-8145be84af5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/colab\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNJWtRpX-jaT",
        "outputId": "c2f0b71a-d9e7-4600-e23d-4187a8f2c290"
      },
      "outputs": [],
      "source": [
        "%pip install monai torch pyimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "WSVwNQZa90js",
        "outputId": "24b64427-1d3a-4af2-c15b-0f0188a1675c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    ToTensord,\n",
        "    AddChanneld,\n",
        "    Spacingd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    Resized,\n",
        "    EnsureChannelFirstd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    Rand3DElasticd,\n",
        "    RandShiftIntensityd,\n",
        "    RandGaussianNoised,\n",
        "    EnsureTyped,\n",
        "    RandFlipd,\n",
        "    RandRotate90d,\n",
        "    #GammaTransformd,\n",
        "    RandZoomd,\n",
        "    Orientationd,\n",
        ")\n",
        "\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.utils import first\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#the data paths are loaded and stored in a dictionary list\n",
        "data_dir = \"/content/drive/My Drive/colab\"\n",
        "root_dir = data_dir\n",
        "\n",
        "train_images = sorted(glob(os.path.join(data_dir, \"KITStrain/case_*/imaging.nii.gz\")))\n",
        "train_labels = sorted(glob(os.path.join(data_dir, \"KITStrain/case_*/aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "val_images = sorted(glob(os.path.join(data_dir, \"KITSval/case_*/imaging.nii.gz\")))\n",
        "val_labels = sorted(glob(os.path.join(data_dir, \"KITSval/case_*/aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "train_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "val_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
        "\n",
        "\n",
        "#the transformations to apply to the images are defined\n",
        "\n",
        "#the transforms are described at the train_transforms block of code\n",
        "orig_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        \n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms = Compose(\n",
        "\n",
        "    [\n",
        "\n",
        "\n",
        "        #load image and label\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        #makes the image format to have furst the num_channel, like from (spatial_dim_1[, spatial_dim_2, …]) to (num_channels, spatial_dim_1[, spatial_dim_2, …])\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        #Ensures that the data are in format that the channel is first -> (C,H,W) instead of (H,W,C)\n",
        "        #EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "        #Must be used before any anisotropiic spatial transform, it assures that the images are in the standar RAS (right,anterior,superior) orientation\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        #Scales the intensity of the image to the given range (normalization)\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=-80, a_max=305,b_min=0.0, b_max=1.0, clip=True),\n",
        "        #it removes the background\n",
        "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
        "\n",
        "        #Resizes the image to the given spatial size\n",
        "        #Resized(keys=['image', 'label'], spatial_size=[128,128,128]),\n",
        "        #Randomly crops the image to the given spatial size taking into account the label\n",
        "        RandCropByPosNegLabeld(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                label_key=\"label\",\n",
        "                spatial_size=(160, 160, 64),\n",
        "                pos=1,\n",
        "                neg=1,\n",
        "                num_samples=4,\n",
        "                image_key=\"image\",\n",
        "                image_threshold=0,\n",
        "            ),\n",
        "        #Randomly elasticly deforms the image\n",
        "        Rand3DElasticd(\n",
        "                keys=[\"image\", \"label\"],\n",
        "                mode=(\"bilinear\", \"nearest\"),\n",
        "                prob=0.5,\n",
        "                sigma_range=(5, 8),\n",
        "                magnitude_range=(50, 150),\n",
        "                spatial_size=(160, 160, 64),\n",
        "                translate_range=(10, 10, 5),\n",
        "                rotate_range=(np.pi/36,np.pi/36, np.pi),\n",
        "                scale_range=(0.1, 0.1, 0.1),\n",
        "                padding_mode=\"zeros\",\n",
        "            ),\n",
        "        #Randomly shifts the intensity of the image\n",
        "        RandShiftIntensityd(\n",
        "                keys=[\"image\"],\n",
        "                offsets=0.10,\n",
        "                prob=0.25,\n",
        "            ),\n",
        "        #Randomly adds gaussian noise to the image\n",
        "        RandGaussianNoised(keys=[\"image\"], prob=0.25, mean=0.0, std=0.1),\n",
        "\n",
        "        #these transformations randomly flip the images in different orientations\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[0],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[1],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[2],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "     \n",
        "        #randomly rotates the images 90 degrees\n",
        "        RandRotate90d(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            prob=0.10,\n",
        "            max_k=3,\n",
        "        ),\n",
        "\n",
        "       #problema -> no hay una transformacion aleatoria de gamma -> o la creo o la aplico a todas??\n",
        "        #GammaTransformd(keys=[\"image\"], gamma_range=(0.7, 1.3)),\n",
        "        \n",
        "        #randomly makes a zoom to the image\n",
        "        RandZoomd(keys=[\"image\", \"label\"], prob=0.1, zoom_range=(0.9,1.1)),\n",
        "        #it ensures that the input data is a pytorch tensor or a numpy array\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        #it transforms the data to a tensor\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "#the transforms are described at the train_transforms block of code\n",
        "val_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        #load image and label\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        #Ensures that the data are in format that the channel is first -> (C,H,W) instead of (H,W,C)\n",
        "        #EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "    \n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        #Scales the intensity of the image to the given range\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=-80, a_max=305,b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
        "\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "#the dataloaders are defined\n",
        "\n",
        "orig_ds = Dataset(data=train_files, transform=orig_transforms)\n",
        "orig_loader = DataLoader(orig_ds, batch_size=2)\n",
        "\n",
        "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=2)\n",
        "\n",
        "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=2)\n",
        "\n",
        "\n",
        "#an image is loaded and compared the original and the train preprocessed image to compare the differences\n",
        "\n",
        "test_patient = first(train_loader)\n",
        "orig_patient = first(orig_loader)\n",
        "\n",
        "\n",
        "print(torch.min(test_patient['image']))\n",
        "print(torch.max(test_patient['image']))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure('test', (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Orig patient')\n",
        "plt.imshow(orig_patient['image'][0, 0, : ,: ,50], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Slice of a patient')\n",
        "plt.imshow(test_patient['image'][0, 0, : ,: ,50], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Label of a patient')\n",
        "plt.imshow(test_patient['label'][0, 0, : ,: ,50])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "WbwL11m490kB",
        "outputId": "6c32abfc-6dd2-4d3b-e907-86f5fdf70e81"
      },
      "outputs": [],
      "source": [
        "#an image is loaded and compared the original and the train preprocessed image to compare the differences\n",
        "\n",
        "#n is used to define which layer from the 3d image show\n",
        "n = 30\n",
        "\n",
        "plt.figure('test', (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Orig patient')\n",
        "plt.imshow(orig_patient['image'][0, 0, : ,: ,n], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Slice of a patient')\n",
        "plt.imshow(test_patient['image'][0, 0, : ,: ,n], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Label of a patient')\n",
        "plt.imshow(test_patient['label'][0, 0, : ,: ,n])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMDF9VUZ90kD",
        "outputId": "67f0d0a5-ddac-449c-d624-315a0657a67d"
      },
      "outputs": [],
      "source": [
        "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,SmartCacheDataset\n",
        "\n",
        "print(\"CREATING TRAIN DS\", flush=True)\n",
        "train_ds = CacheDataset(\n",
        "    data=train_files, transform=train_transforms,\n",
        "    #cache_rate=0.1,\n",
        "    #replace_rate=0.5\n",
        "    )\n",
        "print(len(train_ds))\n",
        "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "print(\"CREATED TRAIN DS\", flush=True)\n",
        "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
        "# to generate 2 x 4 images for network training\n",
        "train_loader = DataLoader(train_ds, batch_size=2, \n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "print(\"CREATED TRAIN DATALOADER\", flush=True)\n",
        "\n",
        "val_ds = CacheDataset(\n",
        "    data=val_files, transform=val_transforms,\n",
        "    #cache_rate=0.1,\n",
        "    #replace_rate=0.5\n",
        "    )\n",
        "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "print(\"CREATED VAL DS\", flush=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, num_workers=2)\n",
        "print(\"CREATED VAL DATALOADER\", flush=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlXZN4ff90kE",
        "outputId": "9acd9807-54b0-4010-df7f-c8a9049fa384"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZA0skh490kF",
        "outputId": "865c1a7d-a72a-45c9-a8c8-de78660609eb"
      },
      "outputs": [],
      "source": [
        "from monai.networks.nets import UNet, UNETR, DynUNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch,SmartCacheDataset\n",
        "from monai.config import print_config\n",
        "from monai.apps import download_and_extract\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from monai.losses import  DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    EnsureType,\n",
        ")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(\n",
        "    dimensions=3, #3 dimensions because the data is 3d\n",
        "    in_channels=1, #the nº of input channels is 1 (the RX intensity)\n",
        "    out_channels=4, #the nº of output channels are 4 (which are the 4 labels to output)\n",
        "    channels=(64, 128, 256, 512), #indicates the nº of channels in each model layer (will have 4 layers with 64,128,256 and 512 channels)\n",
        "    strides=(2, 2, 2, 2), #indicates the stride step (how many pixels jumps the convolution window) (bigger stride makes reduces the output dimension but accelerates the processing and quantity of parameters)\n",
        "    num_res_units=2, #number of residual units\n",
        "    norm=\"INSTANCE\",\n",
        ").to(device) #to device moves the model to the device (for example GPU) to be used for the training\n",
        "\n",
        "#model.load_state_dict(torch.load(best_metric_model_file,map_location=torch.device(device)))\n",
        "\n",
        "print(\"CREATED MODEL\", flush=True)\n",
        "#this loss funcion is a combination between dice and CE (cross entropy)\n",
        "#Dice measures the similarity between 2 images and compares the superposition of segmented regions in the model and in the ground truth\n",
        "#CE compares the output of the model (which is a probability map of the label classes) and compares it with the ground truth\n",
        "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "#the optimizer AdamW is an Adam variation\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-4)\n",
        "#dicemetric is used to evaluate the dice metric of the model\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "\n",
        "max_epochs = 10\n",
        "#how many epochs happens between each evaluation of the model\n",
        "val_interval = 1\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "#these post_pred and post_label are used in the evaluation of the model to see how well the model performs during the training\n",
        "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=4, n_classes=4)])\n",
        "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=4, n_classes=4)])\n",
        "\n",
        "\n",
        "###############################################\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10, flush=True)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\", flush=True)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"label\"].to(device),\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        print(\n",
        "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
        "            f\"train_loss: {loss.item():.4f}\", flush=True)\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\", flush=True)\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"label\"].to(device),\n",
        "                )\n",
        "                roi_size = (160,160, 64)\n",
        "                sw_batch_size = 2\n",
        "                val_outputs = sliding_window_inference(\n",
        "                    val_inputs, roi_size, sw_batch_size, model)\n",
        "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
        "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
        "                # compute metric for current iteration\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            # aggregate the final mean dice result\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            # reset the status for next validation round\n",
        "            dice_metric.reset()\n",
        "\n",
        "            metric_values.append(metric)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                #torch.save(model.state_dict(), os.path.join(\n",
        "                #    root_dir, \"best_metric_model_\"+str(epoch)+\"_\"+str(f\"{metric:.4f}\")+\".pth\"))\n",
        "                print(\"saved new best metric model\", flush=True)\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch}\", flush=True\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "print(epoch_loss_values, flush=True)\n",
        "print(metric_values, flush=True)\n",
        "print(\n",
        "f\"train completed, best_metric: {best_metric:.4f} \"\n",
        "f\"at epoch: {best_metric_epoch}\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "O7G7xlJfJl8A",
        "outputId": "4fdb0d9d-6b84-4cf1-c5bd-07a04d5eac5b"
      },
      "outputs": [],
      "source": [
        "eval_num = val_interval\n",
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Iteration Average Loss\")\n",
        "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHam82hJKXOX"
      },
      "outputs": [],
      "source": [
        "root_dir = data_dir\n",
        "torch.save(model.state_dict(), os.path.join(\n",
        "                    root_dir, \"best_metric_model\"+\".pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "r6zlAaUrR6d9",
        "outputId": "a0518b6e-fbb1-4439-84f6-c7ce52dd3f5d"
      },
      "outputs": [],
      "source": [
        "case_num = 1\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    n = 60\n",
        "    img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
        "    img = val_ds[case_num][\"image\"]\n",
        "    label = val_ds[case_num][\"label\"]\n",
        "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
        "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
        "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model, overlap=0.8)\n",
        "    plt.figure(\"check\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, n], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, n])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"output\")\n",
        "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, n])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "vjHiZ2rqStT_",
        "outputId": "0927e7a1-dbed-4c21-9076-fd883e2f52a9"
      },
      "outputs": [],
      "source": [
        "    n = 50\n",
        "    plt.figure(\"check\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, n], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, n])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"output\")\n",
        "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, n])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "rVY69eJzKXUX",
        "outputId": "76245f06-5b31-497e-f962-b7f726d331ee"
      },
      "outputs": [],
      "source": [
        "case_num = 1\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    n = 30\n",
        "    img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
        "    img = val_ds[case_num][\"image\"]\n",
        "    label = val_ds[case_num][\"label\"]\n",
        "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
        "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
        "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model, overlap=0.8)\n",
        "    plt.figure(\"check\", (18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"image\")\n",
        "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_map[img_name]], cmap=\"gray\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"label\")\n",
        "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_map[img_name]])\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"output\")\n",
        "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, slice_map[img_name]])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "plt.figure('test', (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Orig patient')\n",
        "plt.imshow(orig_patient['image'][0, 0, : ,: ,n], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Slice of a patient')\n",
        "plt.imshow(test_patient['image'][0, 0, : ,: ,n], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Label of a patient')\n",
        "plt.imshow(test_patient['label'][0, 0, : ,: ,n])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqD2aTgq90kG",
        "outputId": "d0cd0f67-a561-4f4c-83ed-238c37dfa868"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNETR(\n",
        "    in_channels=1,\n",
        "    out_channels=4,\n",
        "    img_size=(96, 96, 96),\n",
        "    feature_size=16,\n",
        "    hidden_size=768,\n",
        "    mlp_dim=3072,\n",
        "    num_heads=12,\n",
        "    pos_embed=\"perceptron\",\n",
        "    norm_name=\"instance\",\n",
        "    res_block=True,\n",
        "    dropout_rate=0.0,\n",
        ").to(device)\n",
        "\n",
        "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "def validation(epoch_iterator_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in epoch_iterator_val:\n",
        "            val_inputs, val_labels = (batch[\"image\"].cpu(), batch[\"label\"].cpu())\n",
        "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
        "            val_labels_list = decollate_batch(val_labels)\n",
        "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
        "            val_outputs_list = decollate_batch(val_outputs)\n",
        "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
        "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
        "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
        "        mean_dice_val = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "    return mean_dice_val\n",
        "\n",
        "\n",
        "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        step += 1\n",
        "        x, y = (batch[\"image\"].cpu , batch[\"label\"].cpu())\n",
        "        logit_map = model(x)\n",
        "        loss = loss_function(logit_map, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss))\n",
        "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
        "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
        "            dice_val = validation(epoch_iterator_val)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            metric_values.append(dice_val)\n",
        "            if dice_val > dice_val_best:\n",
        "                dice_val_best = dice_val\n",
        "                global_step_best = global_step\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\n",
        "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
        "                        dice_val_best, dice_val\n",
        "                    )\n",
        "                )\n",
        "        global_step += 1\n",
        "    return global_step, dice_val_best, global_step_best\n",
        "\n",
        "\n",
        "max_iterations = 5\n",
        "eval_num = 500\n",
        "post_label = AsDiscrete(to_onehot=14)\n",
        "post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "global_step = 0\n",
        "dice_val_best = 0.0\n",
        "global_step_best = 0\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "while global_step < max_iterations:\n",
        "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oQHThZ890kI",
        "outputId": "6990d830-9175-49d3-a0db-2fef5094a053"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNETR(\n",
        "    in_channels=1,\n",
        "    out_channels=14,\n",
        "    img_size=(96, 96, 96),\n",
        "    feature_size=16,\n",
        "    hidden_size=768,\n",
        "    mlp_dim=3072,\n",
        "    num_heads=12,\n",
        "    pos_embed=\"perceptron\",\n",
        "    norm_name=\"instance\",\n",
        "    res_block=True,\n",
        "    dropout_rate=0.0,\n",
        ").to(device)\n",
        "\n",
        "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "def validation(epoch_iterator_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in epoch_iterator_val:\n",
        "            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
        "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
        "            val_labels_list = decollate_batch(val_labels)\n",
        "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
        "            val_outputs_list = decollate_batch(val_outputs)\n",
        "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
        "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
        "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
        "        mean_dice_val = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "    return mean_dice_val\n",
        "\n",
        "\n",
        "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        step += 1\n",
        "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
        "        logit_map = model(x)\n",
        "        loss = loss_function(logit_map, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss))\n",
        "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
        "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
        "            dice_val = validation(epoch_iterator_val)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            metric_values.append(dice_val)\n",
        "            if dice_val > dice_val_best:\n",
        "                dice_val_best = dice_val\n",
        "                global_step_best = global_step\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\n",
        "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
        "                        dice_val_best, dice_val\n",
        "                    )\n",
        "                )\n",
        "        global_step += 1\n",
        "    return global_step, dice_val_best, global_step_best\n",
        "\n",
        "\n",
        "max_iterations = 5\n",
        "eval_num = 500\n",
        "post_label = AsDiscrete(to_onehot=14)\n",
        "post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "global_step = 0\n",
        "dice_val_best = 0.0\n",
        "global_step_best = 0\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "while global_step < max_iterations:\n",
        "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKRtdII990kK"
      },
      "outputs": [],
      "source": [
        "from monai.losses import  DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import UNETR\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    EnsureType,\n",
        ")\n",
        "\n",
        "\n",
        "model = UNETR(\n",
        "    in_channels=1,\n",
        "    out_channels=8,\n",
        "    img_size=(96, 96, 96),\n",
        "    feature_size=16,\n",
        "    hidden_size=768,\n",
        "    mlp_dim=3072,\n",
        "    num_heads=12,\n",
        "    pos_embed=\"perceptron\",\n",
        "    norm_name=\"instance\",\n",
        "    res_block=True,\n",
        "    dropout_rate=0.0,\n",
        ").to(device)\n",
        "\n",
        "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
        "\n",
        "\n",
        "max_epochs = 250\n",
        "val_interval = 1\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=4, n_classes=4)])\n",
        "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=4, n_classes=4)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2knEtTUm90kL",
        "outputId": "d25c2af1-515c-47a4-853f-3176122552b6"
      },
      "outputs": [],
      "source": [
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10, flush=True)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\", flush=True)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"label\"].to(device),\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        print(\n",
        "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
        "            f\"train_loss: {loss.item():.4f}\", flush=True)\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\", flush=True)\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"label\"].to(device),\n",
        "                )\n",
        "                roi_size = (160, 160, 64)\n",
        "                sw_batch_size = 4\n",
        "                val_outputs = sliding_window_inference(\n",
        "                    val_inputs, roi_size, sw_batch_size, model)\n",
        "                val_outputs = [post_pred(i)\n",
        "                                            for i in decollate_batch(val_outputs)]\n",
        "                val_labels = [post_label(i)\n",
        "                                            for i in decollate_batch(val_labels)]\n",
        "                # compute metric for current iteration\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            # aggregate the final mean dice result\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            # reset the status for next validation round\n",
        "            dice_metric.reset()\n",
        "\n",
        "            metric_values.append(metric)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(\n",
        "                    root_dir, \"best_metric_model_\"+str(epoch)+\"_\"+str(f\"{metric:.4f}\")+\".pth\"))\n",
        "                print(\"saved new best metric model\", flush=True)\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch}\", flush=True\n",
        "            )\n",
        "\n",
        "# torch.save(model.state_dict(), os.path.join(\n",
        "#                    root_dir, \"last_model.pth\"))\n",
        "print(epoch_loss_values, flush=True)\n",
        "print(metric_values, flush=True)\n",
        "print(\n",
        "f\"train completed, best_metric: {best_metric:.4f} \"\n",
        "f\"at epoch: {best_metric_epoch}\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMF8DoBy90kM",
        "outputId": "d2e75aa5-f2eb-4ff6-b3f9-d1d12560543c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def validation(epoch_iterator_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in epoch_iterator_val:\n",
        "            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
        "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
        "            val_labels_list = decollate_batch(val_labels)\n",
        "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
        "            val_outputs_list = decollate_batch(val_outputs)\n",
        "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
        "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
        "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
        "        mean_dice_val = dice_metric.aggregate().item()\n",
        "        dice_metric.reset()\n",
        "    return mean_dice_val\n",
        "\n",
        "\n",
        "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        step += 1\n",
        "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
        "        logit_map = model(x)\n",
        "        loss = loss_function(logit_map, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss))\n",
        "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
        "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
        "            dice_val = validation(epoch_iterator_val)\n",
        "            epoch_loss /= step\n",
        "            epoch_loss_values.append(epoch_loss)\n",
        "            metric_values.append(dice_val)\n",
        "            if dice_val > dice_val_best:\n",
        "                dice_val_best = dice_val\n",
        "                global_step_best = global_step\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\n",
        "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
        "                        dice_val_best, dice_val\n",
        "                    )\n",
        "                )\n",
        "        global_step += 1\n",
        "    return global_step, dice_val_best, global_step_best\n",
        "\n",
        "\n",
        "max_iterations = 50\n",
        "eval_num = 500\n",
        "post_label = AsDiscrete(to_onehot=14)\n",
        "post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "global_step = 0\n",
        "dice_val_best = 0.0\n",
        "global_step_best = 0\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "while global_step < max_iterations:\n",
        "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
        "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxUP3x-990kP",
        "outputId": "26d777c9-06c6-4183-c9fc-e30c38a6e8a5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandShiftIntensityd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RandRotate90d,\n",
        "    AddChanneld,\n",
        "    ToTensord,\n",
        "\n",
        "    LoadImage,\n",
        "    ToTensor,\n",
        "    AddChannel,\n",
        "    Resized,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import UNETR\n",
        "\n",
        "from monai.data import (\n",
        "    DataLoader,\n",
        "    CacheDataset,\n",
        "    load_decathlon_datalist,\n",
        "    decollate_batch,\n",
        ")\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFLHsTGs90kQ",
        "outputId": "7123659e-bd76-451f-b0ac-cfebd9fe6854"
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"C:\\\\Users\\\\jm\\\\Desktop\\\\monai\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo1kOT5m90kQ"
      },
      "outputs": [],
      "source": [
        "orig_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        AddChanneld(keys=[\"image\", \"label\"]),\n",
        "\n",
        "        ToTensord(keys=[\"image\", \"label\"])\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"image\"],\n",
        "            a_min=-175,\n",
        "            a_max=250,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "        RandCropByPosNegLabeld(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            label_key=\"label\",\n",
        "            spatial_size=(96, 96, 96),\n",
        "            pos=1,\n",
        "            neg=1,\n",
        "            num_samples=4,\n",
        "            image_key=\"image\",\n",
        "            image_threshold=0,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[0],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[1],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandFlipd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            spatial_axis=[2],\n",
        "            prob=0.10,\n",
        "        ),\n",
        "        RandRotate90d(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            prob=0.10,\n",
        "            max_k=3,\n",
        "        ),\n",
        "        RandShiftIntensityd(\n",
        "            keys=[\"image\"],\n",
        "            offsets=0.10,\n",
        "            prob=0.50,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "        Spacingd(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            pixdim=(1.5, 1.5, 2.0),\n",
        "            mode=(\"bilinear\", \"nearest\"),\n",
        "        ),\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CONwQD2P90kR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "data_dir = \"C:\\\\Users\\\\jm\\\\Desktop\\\\folder\"\n",
        "\n",
        "train_images = sorted(glob(os.path.join(data_dir, \"KITStrain\\\\case_*\\\\imaging.nii.gz\")))\n",
        "train_labels = sorted(glob(os.path.join(data_dir, \"KITStrain\\\\case_*\\\\aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "val_images = sorted(glob(os.path.join(data_dir, \"KITSval\\\\case_*\\\\imaging.nii.gz\")))\n",
        "val_labels = sorted(glob(os.path.join(data_dir, \"KITSval\\\\case_*\\\\aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "train_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "val_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(val_images, val_labels)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CC-tZI_90kS"
      },
      "outputs": [],
      "source": [
        "from monai.data import Dataset, DataLoader\n",
        "from monai.utils import first\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#orig_transforms = Compose(\n",
        "#    [\n",
        "#        LoadImaged(keys=[\"image\", \"label\"])\n",
        "        #AddChanneld(keys=[\"image\", \"label\"]),\n",
        "\n",
        "        #ToTensord(keys=[\"image\", \"label\"])\n",
        "#    ]\n",
        "#)\n",
        "\n",
        "#train_files = [{\"image\" : \"C:\\\\Users\\\\Desktop\\\\KITS-21\\\\case_00001\\\\imaging.nii.gz\", \"label\" : \"C:\\\\Users\\\\Desktop\\\\KITS-21\\\\case_00001\\\\aggregated_MAJ_seg.nii.gz\"}]\n",
        "\n",
        "orig_ds = Dataset(data=train_files, transform=orig_transforms)\n",
        "orig_loader = DataLoader(orig_ds, batch_size=1)#, num_workers=4, shuffle=True)\n",
        "\n",
        "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=1)#, num_workers=4, shuffle=True)\n",
        "\n",
        "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=1)#, num_workers=4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HODwkswO90kS",
        "outputId": "578b176a-4c62-4283-db7a-d7356a054152"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import pprint\n",
        "\n",
        "import torch\n",
        "import torchio as tio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "torch.manual_seed(14041931)\n",
        "\n",
        "print('TorchIO version:', tio.__version__)\n",
        "print('Last run on', time.ctime())\n",
        "\n",
        "fpg = tio.datasets.FPG()\n",
        "print('Sample subject:', fpg)\n",
        "show_fpg(fpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqALtvF-90kT",
        "outputId": "62849810-b673-442e-d58b-ec6b633d6a08"
      },
      "outputs": [],
      "source": [
        "test_patient = first(train_loader)\n",
        "orig_patient = first(orig_loader)\n",
        "\n",
        "plt.figure(\"test\", (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"orig patient image\")\n",
        "plt.imshow(orig_patient[\"image\"][0, 0, :, :, 30], cmap=\"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"slice of a patient\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlhkGMpv90kU",
        "outputId": "fdc41adf-544b-4b2d-da7b-23e2d98a52c0"
      },
      "outputs": [],
      "source": [
        "plt.figure('test', (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Orig patient')\n",
        "plt.imshow(orig_patient['image'][0, 0, : ,: ,30], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Slice of a patient')\n",
        "plt.imshow(test_patient['image'][0, 0, : ,: ,30], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Label of a patient')\n",
        "plt.imshow(test_patient['label'][0, 0, : ,: ,30])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjIr64DS90kf",
        "outputId": "1807fb8f-bc49-4833-f445-042a806bc835"
      },
      "outputs": [],
      "source": [
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "m = first(train_files)\n",
        "m = \"C:\\\\Users\\\\Desktop\\\\KITS-21\\\\case_00000\\\\imaging.nii.gz\"\n",
        "image = sitk.ReadImage(m)\n",
        "image_array = sitk.GetArrayFromImage(image)\n",
        "n = image_array[300,:,:]\n",
        "img = sitk.ReadImage(n)\n",
        "plt.subplot(121)\n",
        "plt.imshow(img)\n",
        "plt.title('Imagen')\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch9S8dhK90kg"
      },
      "outputs": [],
      "source": [
        "# load the images\n",
        "# do any transforms\n",
        "# need to convert them into troch tensors\n",
        "\n",
        "orig_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        \n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "orig_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        #AddChanneld(keys=[\"image\", \"label\"]),\n",
        "\n",
        "        #ToTensord(keys=[\"image\", \"label\"])\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "        ScaleIntensityRanged(keys='image', a_min=-200, a_max=200, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
        "        Resized(keys=['image', 'label'], spatial_size=[128,128,128]),\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "val_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "        ScaleIntensityRanged(keys='image', a_min=-200, a_max=200, b_min=0.0, b_max=1.0, clip=True),\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G9O5m-p90kh",
        "outputId": "6140fe07-bb9b-41a6-f5df-391988337938"
      },
      "outputs": [],
      "source": [
        "import monai\n",
        "import os\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    EnsureChannelFirstd,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    RandShiftIntensityd,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RandRotate90d,\n",
        "    AddChanneld,\n",
        "    ToTensord,\n",
        "\n",
        "    LoadImage,\n",
        "    ToTensor,\n",
        "    AddChannel,\n",
        "    Resized,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.networks.nets import UNETR\n",
        "\n",
        "from monai.data import (\n",
        "    DataLoader,\n",
        "    CacheDataset,\n",
        "    load_decathlon_datalist,\n",
        "    decollate_batch,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "filename = os.path.join(\"C:\\\\Users\\\\Desktop\", \"MONAI-logo_color.png\")\n",
        "monai.apps.download_url(\"https://monai.io/assets/img/MONAI-logo_color.png\", filepath=filename)\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=\"image\", image_only=True, ensure_channel_first=True, dtype=np.uint8),\n",
        "        Resized(keys=\"image\", spatial_size=[60, 64]),\n",
        "    ]\n",
        ")\n",
        "test_data = {\"image\": filename}\n",
        "result = transform(test_data)\n",
        "print(result[\"image\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFJVjTAw90ki",
        "outputId": "4bca8fc5-4188-4434-89de-68dff0129af3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    ToTensord,\n",
        "    AddChanneld,\n",
        "    Spacingd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    Resized,\n",
        "\n",
        ")\n",
        "\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.utils import first\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_dir = \"C:\\\\Users\\\\jm\\\\Desktop\\\\folder\"\n",
        "\n",
        "train_images = sorted(glob(os.path.join(data_dir, \"KITStrain\\\\case_*\\\\imaging.nii.gz\")))\n",
        "train_labels = sorted(glob(os.path.join(data_dir, \"KITStrain\\\\case_*\\\\aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "val_images = sorted(glob(os.path.join(data_dir, \"KITSval\\\\case_*\\\\imaging.nii.gz\")))\n",
        "val_labels = sorted(glob(os.path.join(data_dir, \"KITSval\\\\case_*\\\\aggregated_MAJ_seg.nii.gz\")))\n",
        "\n",
        "train_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
        "val_files = [{\"image\": image_name, 'label': label_name} for image_name, label_name in zip(val_images, val_labels)]\n",
        "\n",
        "\n",
        "\n",
        "orig_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        \n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "        ScaleIntensityRanged(keys='image', a_min=-200, a_max=200, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
        "        Resized(keys=['image', 'label'], spatial_size=[128,128,128]),\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "val_transforms = Compose(\n",
        "\n",
        "    [\n",
        "        LoadImaged(keys=['image', 'label']),\n",
        "        AddChanneld(keys=['image', 'label']),\n",
        "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2)),\n",
        "        ScaleIntensityRanged(keys='image', a_min=-200, a_max=200, b_min=0.0, b_max=1.0, clip=True),\n",
        "        ToTensord(keys=['image', 'label'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "orig_ds = Dataset(data=train_files, transform=orig_transforms)\n",
        "orig_loader = DataLoader(orig_ds, batch_size=1)\n",
        "\n",
        "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=1)\n",
        "\n",
        "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=1)\n",
        "\n",
        "\n",
        "\n",
        "test_patient = first(train_loader)\n",
        "orig_patient = first(orig_loader)\n",
        "\n",
        "\n",
        "print(torch.min(test_patient['image']))\n",
        "print(torch.max(test_patient['image']))\n",
        "\n",
        "\n",
        "\n",
        "plt.figure('test', (12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Orig patient')\n",
        "plt.imshow(orig_patient['image'][0, 0, : ,: ,50], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Slice of a patient')\n",
        "plt.imshow(test_patient['image'][0, 0, : ,: ,50], cmap= \"gray\")\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Label of a patient')\n",
        "plt.imshow(test_patient['label'][0, 0, : ,: ,50])\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b5b9f0034e9a378fe5e305d554d74025f879acf88fdbf6c43244c57b4197fe9a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
